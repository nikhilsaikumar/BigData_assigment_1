{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing PySpark\n",
    "\n",
    "To begin, it is often good practice to create a conda environment for a project. These environments have their own installed set of packages; therefore, you can keep the 'base' environment as a 'clean' environment that has only the basics, and then create seperate environments for any major projects (or type of work). \n",
    "\n",
    "Each environment can also have its own python version.\n",
    "\n",
    "Let's begin by creating a new conda environment called pyspark that has python version 3.10 (note: 3.11 was recently released, but the current version of pyspark will not run in version 3.11 (as of March, 2023)). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Java 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark requires java 8, so we need to install the java runtime environment first. Follow the instructions to download and install java from https://www.java.com/en/download/\n",
    "\n",
    "NOTE: If you are using a M1 or M2 mac, you will need to take one of two approaches: \n",
    "1. Install the Rosetta 2 emulator (https://support.apple.com/en-us/HT211861) and then install java 8.\n",
    "2. RECOMMENDED: Install Azul Zulu JDK 8 (https://www.azul.com/downloads/zulu-community/?version=java-8-lts&os=macos&architecture=arm-64-bit&package=jdk) and then install java 8.\n",
    "   1. Make sure you set your JAVA HOME environment variable to the location of the JDK (e.g. /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home)\n",
    "      * edit .zshrc and add the following lines\n",
    "        * ```export JAVA_HOME=/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home```\n",
    "        * ```export PATH=$JAVA_HOME/bin:$PATH```\n",
    "\n",
    "For Windows or Mac's with x86 processors, you can install java 8 from https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\n",
    "\n",
    "Make sure to test that you have java 8 installed by running the following command in your terminal:\n",
    "\n",
    "    java -version\n",
    "\n",
    "If you get an error or a difference version, you may need to alter/update your environment variables.\n",
    "* On windows, you can access your environment variables by searching for 'environment variables' in the start menu.\n",
    "  * Also, see here https://docs.oracle.com/en/database/oracle/machine-learning/oml4r/1.5.1/oread/creating-and-modifying-environment-variables-on-windows.html\n",
    "* On MacOS X86 processors, you can access your environment variables using the termoinal.\n",
    "  * https://phoenixnap.com/kb/set-environment-variable-mac\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a conda environment for your spark coding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```conda create --name spark python=3.10```\n",
    "\n",
    "We then need to activate the environment. This will change the terminal prompt to show the name of the environment.\n",
    "\n",
    "```conda activate spark```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pyspark and findspark\n",
    "\n",
    "Now we can install pyspark and findspark. We will also install jupyter notebook, which is a great tool for running and sharing code. \n",
    "\n",
    "```conda install pyspark findspark jupyterlab```\n",
    "\n",
    "To launch a jupyter notebook, we can simply type ```jupyter notebook``` in the terminal. This notebook will use the 'spark' conda environment that is currently active. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Creating a pyspark jupyter kernal\n",
    "\n",
    "If you wish to have this environment selectable within jupyter lab, you can install the conda environment as a kernel. \n",
    "\n",
    "```python -m ipykernel install --user --name=spark --display-name=\"Python (spark)\"```\n",
    "\n",
    "* NOTE: To verify is the kernel is installed, you can run ```jupyter kernelspec list``` in the terminal. This will list all of the kernels that are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install other packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new 'spark' environment has only the basics installed, so we will need to install the packages we need.\n",
    "\n",
    "For now, we will need to install pandas, matplotlib, jupyter lab, and pyspark (you may need to install more packages later; but you should know how to do this by now)\n",
    "\n",
    "```conda install -c conda-forge pandas=1.5.3 matplotlib sparkmagic```\n",
    "\n",
    "#### NOTE: Pandas is transitioning to PyArrrow - at the time of this writing, Pandas 2.0 is released and the version tha Conda installs. However, PySpark does not yet seem to support pandas with PyArrow, so we need to install Pandas 1.5.3. Look for this to change soon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing you PySpark installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to create and use a spark session. Make sure you download the BostonHousing.csv dataset (available in canvas and in the class github repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m findspark\u001b[39m.\u001b[39minit()\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession;\n\u001b[1;32m----> 6\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[4]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mISM6562 Spark App01\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49menableHiveSupport()\u001b[39m.\u001b[39;49mgetOrCreate();\n\u001b[0;32m      8\u001b[0m \u001b[39m# note: If you have multiple spark sessions running (like from a previous notebook you've run), \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# this spark session webUI will be on a different port than the default (4040). One way to \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# identify this part is with the following line. If there was only one spark session running, \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m spark_session_port \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39muiWebUrl\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\18138\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    229\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mc:\\Users\\18138\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\18138\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mc:\\Users\\18138\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 339\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    340\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\18138\\anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    111\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession;\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"ISM6562 Spark App01\").enableHiveSupport().getOrCreate();\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link \"Spark UI\" that is displayed after running the code above. If everything is working, you should see that your spark session is running.\n",
    "\n",
    "*NOTE: Keep in mind that when you're running a notebook, it's not just the code in the notebook that's running. Until you hit the shutdown or restart kernal button, the spark session is still running. So if you run the above code, you'll see that the port is 4041. If you run the code again, it will be 4042. If you run the code again, it will be 4043. And so on.*\n",
    "\n",
    "If you need stop the spark session you can therefore restart/stop the kernal or run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
